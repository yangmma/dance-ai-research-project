@inproceedings{Kim2022,
   author = {Jinwoo Kim and Heeseok Oh and Seongjean Kim and Hoseok Tong and Sanghoon Lee},
   doi = {10.1109/CVPR52688.2022.00348},
   journal = {2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
   keywords = {Training;Measurement;Humanities;Computer vision;Computational modeling;Computer architecture;Transformers;Image and video synthesis and generation; Action and event recognition; Deep learning architectures and techniques; Vision + X},
   pages = {3480-3490},
   title = {A Brand New Dance Partner: Music-Conditioned Pluralistic Dancing Controlled by Multiple Dance Genres},
   year = {2022},
}
@article{Holden2016,
   abstract = {We present a framework to synthesize character movements based on high level parameters, such that the produced movements respect the manifold of human motion, trained on a large motion capture dataset. The learned motion manifold, which is represented by the hidden units of a convolutional autoencoder, represents motion data in sparse components which can be combined to produce a wide range of complex movements. To map from high level parameters to the motion manifold, we stack a deep feedforward neural network on top of the trained autoencoder. This network is trained to produce realistic motion sequences from parameters such as a curve over the terrain that the character should follow, or a target location for punching and kicking. The feedforward control network and the motion manifold are trained independently, allowing the user to easily switch between feedforward networks according to the desired interface, without re-training the motion manifold. Once motion is generated it can be edited by performing optimization in the space of the motion manifold. This allows for imposing kinematic constraints, or transforming the style of the motion, while ensuring the edited motion remains natural. As a result, the system can produce smooth, high quality motion sequences without any manual pre-processing of the training data.},
   author = {Daniel Holden and Jun Saito and Taku Komura},
   city = {New York, NY, USA},
   doi = {10.1145/2897824.2925975},
   issn = {0730-0301},
   issue = {4},
   journal = {ACM Trans. Graph.},
   keywords = {autoencoder,character animation,convolutional neural networks,deep learning,human motion,manifold learning},
   month = {7},
   publisher = {Association for Computing Machinery},
   title = {A deep learning framework for character motion synthesis and editing},
   volume = {35},
   url = {https://doi.org/10.1145/2897824.2925975},
   year = {2016},
}
@article{Li2021,
   abstract = {We present AIST++, a new multi-modal dataset of 3D dance motion and music, along with FACT, a Full-Attention Cross-modal Transformer network for generating 3D dance motion conditioned on music. The proposed AIST++ dataset contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance genres with multi-view videos with known camera poses -- the largest dataset of this kind to our knowledge. We show that naively applying sequence models such as transformers to this dataset for the task of music conditioned 3D motion generation does not produce satisfactory 3D motion that is well correlated with the input music. We overcome these shortcomings by introducing key changes in its architecture design and supervision: FACT model involves a deep cross-modal transformer block with full-attention that is trained to predict $N$ future motions. We empirically show that these changes are key factors in generating long sequences of realistic dance motion that are well-attuned to the input music. We conduct extensive experiments on AIST++ with user studies, where our method outperforms recent state-of-the-art methods both qualitatively and quantitatively.},
   author = {Ruilong Li and Shan Yang and David A. Ross and Angjoo Kanazawa},
   month = {1},
   title = {AI Choreographer: Music Conditioned 3D Dance Generation with AIST++},
   url = {http://arxiv.org/abs/2101.08779},
   year = {2021},
}
@inproceedings{Trajkova2023,
   abstract = {In this demonstration, we present a holographic projected version of LuminAI, which is an interactive art installation that allows participants to collaborate with an AI dance partner by improvising movements together. By utilizing a mix of a top-down and bottom-up approach, we seek to understand embodied co-creativity in an improvisational dance setting to better develop the design of the modular AI agent to creatively collaborate with a dancer. The purpose of this demonstration is to describe the five-module agent design and investigate how we can design an immersive experience that is design-efficient, portable, light, and duo-user participation. Through this installation in an imitated black box space, audience members and dancers engage in an immersive co-creative dance experience, inspiring discussion on the limitless applications of dance and technology in the realms of learning, training, and creativity.},
   author = {Milka Trajkova and Manoj Deshpande and Andrea Knowlton and Cassandra Monden and Duri Long and Brian Magerko},
   city = {New York, NY, USA},
   doi = {10.1145/3563703.3596658},
   isbn = {9781450398985},
   journal = {Companion Publication of the 2023 ACM Designing Interactive Systems Conference},
   keywords = {AI agents,co-creative AI,co-creative agents,co-creativity,dance improvisation},
   pages = {274-278},
   publisher = {Association for Computing Machinery},
   title = {AI Meets Holographic Pepper’s Ghost: A Co-Creative Public Dance Experience},
   url = {https://doi.org/10.1145/3563703.3596658},
   year = {2023},
}
@article{Siyao2023,
   author = {Li Siyao and Weijiang Yu and Tianpei Gu and Chunze Lin and Quan Wang and Chen Qian and Chen Change Loy and Ziwei Liu},
   doi = {10.1109/TPAMI.2023.3319435},
   issue = {12},
   journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
   keywords = {Humanities;Three-dimensional displays;Codes;Transformers;Avatars;Rhythm;Encoding;3D human motion;dance generation;GPT;multi-modal;VQ-VAE},
   note = {model: VQVAE into a Transformer<br/><br/>VQVAE has the advantage of extracting codes (features) from dances and allowing those codes to be reused for other dances.<br/><br/>Training the VQVAE decoder cannot be trained to generate rot mat by itself.},
   pages = {14192-14207},
   title = {Bailando++: 3D Dance GPT With Choreographic Memory},
   volume = {45},
   year = {2023},
}
@article{Pettee2019,
   author = {Mariel Pettee and Chase Shimmin and Douglas Duhaime and Ilya Vidrin},
   title = {Beyond Imitation: Generative and Variational Choreography via Machine Learning},
   year = {2019},
}
@article{Pettee2019,
   author = {Mariel Pettee and Chase Shimmin and Douglas Duhaime and Ilya Vidrin},
   title = {Beyond Imitation: Generative and Variational Choreography via Machine Learning},
   year = {2019},
}
@article{Le2023,
   abstract = {Music-driven group choreography poses a considerable challenge but holds significant potential for a wide range of industrial applications. The ability to generate synchronized and visually appealing group dance motions that are aligned with music opens up opportunities in many fields such as entertainment, advertising, and virtual performances. However, most of the recent works are not able to generate high-fidelity long-term motions, or fail to enable controllable experience. In this work, we aim to address the demand for high-quality and customizable group dance generation by effectively governing the consistency and diversity of group choreographies. In particular, we utilize a diffusion-based generative approach to enable the synthesis of flexible number of dancers and long-term group dances, while ensuring coherence to the input music. Ultimately, we introduce a Group Contrastive Diffusion (GCD) strategy to enhance the connection between dancers and their group, presenting the ability to control the consistency or diversity level of the synthesized group animation via the classifier-guidance sampling technique. Through intensive experiments and evaluation, we demonstrate the effectiveness of our approach in producing visually captivating and consistent group dance motions. The experimental results show the capability of our method to achieve the desired levels of consistency and diversity, while maintaining the overall quality of the generated group choreography.},
   author = {Nhat Le and Tuong Do and Khoa Do and Hien Nguyen and Erman Tjiputra and Quang D Tran and Anh Nguyen},
   city = {New York, NY, USA},
   doi = {10.1145/3618356},
   issn = {0730-0301},
   issue = {6},
   journal = {ACM Trans. Graph.},
   keywords = {diffusion models,group choreography animation,group motion synthesis,machine learning},
   month = {12},
   publisher = {Association for Computing Machinery},
   title = {Controllable Group Choreography Using Contrastive Diffusion},
   volume = {42},
   url = {https://doi.org/10.1145/3618356},
   year = {2023},
}
@inproceedings{Tang2018,
   abstract = {Dance is greatly influenced by music. Studies on how to synthesize music-oriented dance choreography can promote research in many fields, such as dance teaching and human behavior research. Although considerable effort has been directed toward investigating the relationship between music and dance, the synthesis of appropriate dance choreography based on music remains an open problem. There are two main challenges: 1) how to choose appropriate dance figures, i.e., groups of steps that are named and specified in technical dance manuals, in accordance with music and 2) how to artistically enhance choreography in accordance with music. To solve these problems, in this paper, we propose a music-oriented dance choreography synthesis method using a long short-term memory (LSTM)-autoencoder model to extract a mapping between acoustic and motion features. Moreover, we improve our model with temporal indexes and a masking method to achieve better performance. Because of the lack of data available for model training, we constructed a music-dance dataset containing choreographies for four types of dance, totaling 907,200 frames of 3D dance motions and accompanying music, and extracted multidimensional features for model training. We employed this dataset to train and optimize the proposed models and conducted several qualitative and quantitative experiments to select the best-fitted model. Finally, our model proved to be effective and efficient in synthesizing valid choreographies that are also capable of musical expression.},
   author = {Taoran Tang and Jia Jia and Hanyang Mao},
   city = {New York, NY, USA},
   doi = {10.1145/3240508.3240526},
   isbn = {9781450356657},
   journal = {Proceedings of the 26th ACM International Conference on Multimedia},
   keywords = {3d motion capture,autoencoder,lstm,motion synthesis,music-dance dataset},
   pages = {1598-1606},
   publisher = {Association for Computing Machinery},
   title = {Dance with Melody: An LSTM-autoencoder Approach to Music-oriented Dance Synthesis},
   url = {https://doi.org/10.1145/3240508.3240526},
   year = {2018},
}
@inproceedings{Yao2023,
   abstract = {Recently, digital humans for interpersonal interaction in virtual environments have gained significant attention. In this paper, we introduce a novel multi-dancer synthesis task called partner dancer generation, which involves synthesizing virtual human dancers capable of performing dance with users. The task aims to control the pose diversity between the lead dancer and the partner dancer. The core of this task is to ensure the controllable diversity of the generated partner dancer while maintaining temporal coordination with the lead dancer. This scenario varies from earlier research in generating dance motions driven by music, as our emphasis is on automatically designing partner dancer postures according to pre-defined diversity, the pose of lead dancer, as well as the accompanying tunes. To achieve this objective, we propose a three-stage framework called Dance-with-You (DanY). Initially, we employ a 3D Pose Collection stage to collect a wide range of basic dance poses as references for motion generation. Then, we introduce a hyper-parameter that coordinates the similarity between dancers by masking poses to prevent the generation of sequences that are over-diverse or consistent. To avoid the rigidity of movements, we design a Dance Pre-generated stage to pre-generate these masked poses instead of filling them with zeros. After that, a Dance Motion Transfer stage is adopted with leader sequences and music, in which a multi-conditional sampling formula is rewritten to transfer the pre-generated poses into a sequence with a partner style. In practice, to address the lack of multi-person datasets, we introduce AIST-M, a new dataset for partner dancer generation, which is publicly availiable at https://github.com/JJessicaYao/AIST-M-Dataset. Comprehensive evaluations on our AIST-M dataset demonstrate that the proposed DanY can synthesize satisfactory partner dancer results with controllable diversity.},
   author = {Siyue Yao and Mingjie Sun and Bingliang Li and Fengyu Yang and Junle Wang and Ruimao Zhang},
   doi = {10.1145/3581783.3612046},
   isbn = {9798400701085},
   journal = {MM 2023 - Proceedings of the 31st ACM International Conference on Multimedia},
   keywords = {diffusion model,diversity controllability,partner dancer synthesis},
   month = {10},
   note = {Implementation: <br/>- 3 stages:<br/><br/>1. VQVAE (same as the bailando model I believe)<br/>   Take dance sequence: S, where S -> R^(T, J, 3), T being time series dimension, and J is the joint information dimension. Turn S into Q, where Q -> R6(T, F), where T is time series dim, and F is features dim. TLDR; turn complicated 3D dance joint position data into pose codes that can be learned easier by the following blocks.<br/><br/>2. Dance Pregen<br/>   Take the lead dancer dance sequence, randomly mask out X number of frames using a preset value. Fill the frames with noise and denoise with a [Transformer?] U-Net. Then output to next stage. U-Net is made up of 2 Transformer and 5 resnets as encoder and decoder.<br/><br/>3. Motion Transfer<br/>   Use the same feature selection (mask) module to select the lead features, and masked music features, using the reverse mask. Then, take both feature sets, and use as one condition for the denoise U-Net and the previous layer's output as another condition. Loss for the U-Net: pg 5.<br/>},
   pages = {8504-8514},
   publisher = {Association for Computing Machinery, Inc},
   title = {Dance with You: The Diversity Controllable Dancer Generation via Diffusion Models},
   year = {2023},
}
@misc{Li2023,
   author = {Buyu Li and Yongchi Zhao and Zhelun Shi and Lu Sheng},
   title = {DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer},
   year = {2023},
}
@article{Lee2019,
   author = {Hsin-Ying Lee and Xiaodong Yang and Ming-Yu Liu and Ting-Chun Wang and Yu-Ding Lu and Ming-Hsuan Yang and Jan Kautz},
   title = {Dancing to Music},
   year = {2019},
}
@article{Sun2021,
   author = {Guofei Sun and Yongkang Wong and Zhiyong Cheng and Mohan S Kankanhalli and Weidong Geng and Xiangdong Li},
   doi = {10.1109/TMM.2020.2981989},
   journal = {IEEE Transactions on Multimedia},
   keywords = {Generators;Feature extraction;Task analysis;Correlation;Three-dimensional displays;Music;Deep learning;Music-driven dance choreography;adversarial learning;cross-modal association},
   pages = {497-509},
   title = {DeepDance: Music-to-Dance Motion Choreography With Adversarial Learning},
   volume = {23},
   year = {2021},
}
@inproceedings{Long2019,
   abstract = {Artificial intelligence (AI) is becoming increasingly pervasive in our everyday lives. There are consequently many common misconceptions about what AI is, what it is capable of, and how it works. Compounding the issue, opportunities to learn about AI are often limited to audiences who already have access to and knowledge about technology. Increasing access to AI in public spaces has the potential to broaden public AI literacy, and experiences involving co-creative (i.e. collaboratively creative) AI are particularly well-suited for engaging a broad range of participants. This paper explores how to design co-creative AI for public interaction spaces, drawing both on existing literature and our own experiences designing co-creative AI for public venues. It presents a set of design principles that can aid others in the development of co-creative AI for public spaces as well as guide future research agendas.},
   author = {Duri Long and Mikhail Jacob and Brian Magerko},
   city = {New York, NY, USA},
   doi = {10.1145/3325480.3325504},
   isbn = {9781450359177},
   journal = {Proceedings of the 2019 Conference on Creativity and Cognition},
   keywords = {co-creative ai,collaboration,human-centered ai,public displays,reflection on design processes},
   pages = {271-284},
   publisher = {Association for Computing Machinery},
   title = {Designing Co-Creative AI for Public Spaces},
   url = {https://doi.org/10.1145/3325480.3325504},
   year = {2019},
}
@inproceedings{Qi2023,
   abstract = {When hearing music, it is natural for people to dance to its rhythm. Automatic dance generation, however, is a challenging task due to the physical constraints of human motion and rhythmic alignment with target music. Conventional autoregressive methods introduce compounding errors during sampling and struggle to capture the long-term structure of dance sequences. To address these limitations, we present a novel cascaded motion diffusion model, DiffDance, designed for high-resolution, long-form dance generation. This model comprises a music-to-dance diffusion model and a sequence super-resolution diffusion model. To bridge the gap between music and motion for conditional generation, DiffDance employs a pretrained audio representation learning model to extract music embeddings and further align its embedding space to motion via contrastive loss. During training our cascaded diffusion model, we also incorporate multiple geometric losses to constrain the model outputs to be physically plausible and add a dynamic loss weight that adaptively changes over diffusion timesteps to facilitate sample diversity. Through comprehensive experiments performed on the benchmark dataset AIST++, we demonstrate that DiffDance is capable of generating realistic dance sequences that align effectively with the input music. These results are comparable to those achieved by state-of-the-art autoregressive methods.},
   author = {Qiaosong Qi and Le Zhuo and Aixi Zhang and Yue Liao and Fei Fang and Si Liu and Shuicheng Yan},
   city = {New York, NY, USA},
   doi = {10.1145/3581783.3612307},
   isbn = {9798400701085},
   journal = {Proceedings of the 31st ACM International Conference on Multimedia},
   keywords = {conditional generation,diffusion model,multimodal learning,music-to-dance},
   note = {Model: cascaded (likely unet upsampling) motion duffusion model<br/><br/>Dataset: AIST++<br/><br/>- autoregressive methods introduce compounding errors during sampling and struggle to capture long-term structure.<br/>- Uses wave2CLIP instead of Librosa for music feature extraction},
   pages = {1374-1382},
   publisher = {Association for Computing Machinery},
   title = {DiffDance: Cascaded Human Motion Diffusion Model for Dance Generation},
   url = {https://doi.org/10.1145/3581783.3612307},
   year = {2023},
}
@article{Tseng2022,
   abstract = {Dance is an important human art form, but creating new dances can be difficult and time-consuming. In this work, we introduce Editable Dance GEneration (EDGE), a state-of-the-art method for editable dance generation that is capable of creating realistic, physically-plausible dances while remaining faithful to the input music. EDGE uses a transformer-based diffusion model paired with Jukebox, a strong music feature extractor, and confers powerful editing capabilities well-suited to dance, including joint-wise conditioning, and in-betweening. We introduce a new metric for physical plausibility, and evaluate dance quality generated by our method extensively through (1) multiple quantitative metrics on physical plausibility, beat alignment, and diversity benchmarks, and more importantly, (2) a large-scale user study, demonstrating a significant improvement over previous state-of-the-art methods. Qualitative samples from our model can be found at our website.},
   author = {Jonathan Tseng and Rodrigo Castellon and C. Karen Liu},
   month = {11},
   title = {EDGE: Editable Dance Generation From Music},
   url = {http://arxiv.org/abs/2211.10658},
   year = {2022},
}
@inproceedings{Wallace2023,
   abstract = {What expectations exist in the minds of dancers when interacting with a generative machine learning model? During two workshop events, experienced dancers explore these expectations through improvisation and role-play, embodying an imagined AI-dancer. The dancers explored how intuited flow, shared images, and the concept of a human replica might work in their imagined AI-human interaction. Our findings challenge existing assumptions about what is desired from generative models of dance, such as expectations of realism, and how such systems should be evaluated. We further advocate that such models should celebrate non-human artefacts, focus on the potential for serendipitous moments of discovery, and that dance practitioners should be included in their development. Our concrete suggestions show how our findings can be adapted into the development of improved generative and interactive machine learning models for dancers’ creative practice.},
   author = {Benedikte Wallace and Clarice Hilton and Kristian Nymoen and Jim Torresen and Charles Patrick Martin and Rebecca Fiebrink},
   city = {New York, NY, USA},
   doi = {10.1145/3591196.3593336},
   isbn = {9798400701801},
   journal = {Proceedings of the 15th Conference on Creativity and Cognition},
   keywords = {dance,embodiment,generative AI,reflexive thematic analysis},
   pages = {454-464},
   publisher = {Association for Computing Machinery},
   title = {Embodying an Interactive AI for Dance Through Movement Ideation},
   url = {https://doi.org/10.1145/3591196.3593336},
   year = {2023},
}
@article{Men2022,
   abstract = {Creating realistic characters that can react to the users' or another character's movement can benefit computer graphics, games and virtual reality hugely. However, synthesizing such reactive motions in human-human interactions is a challenging task due to the many different ways two humans can interact. While there are a number of successful researches in adapting the generative adversarial network (GAN) in synthesizing single human actions, there are very few on modeling human-human interactions. In this paper, we propose a semi-supervised GAN system that synthesizes the reactive motion of a character given the active motion from another character. Our key insights are twofold. First, to effectively encode the complicated spatial-temporal information of a human motion, we empower the generator with a part-based long short-term memory (LSTM) module, such that the temporal movement of different limbs can be effectively modeled. We further include an attention module such that the temporal significance of the interaction can be learned, which enhances the temporal alignment of the active-reactive motion pair. Second, as the reactive motion of different types of interactions can be significantly different, we introduce a discriminator that not only tells if the generated movement is realistic or not, but also tells the class label of the interaction. This allows the use of such labels in supervising the training of the generator. We experiment with the SBU, the HHOI and the 2C datasets. The high quality of the synthetic motion demonstrates the effective design of our generator, and the discriminability of the synthesis also demonstrates the strength of our discriminator.},
   author = {Qianhui Men and Hubert P H Shum and Edmond S L Ho and Howard Leung},
   doi = {10.1016/j.cag.2021.09.014},
   journal = {Computers & Graphics},
   keywords = {Attention,Generative adversarial network,Reactive motion synthesis},
   pages = {634-645},
   title = {GAN-based reactive motion synthesis with class-aware discriminators for human-human interaction},
   volume = {102},
   url = {https://doi.org/10.1016/j.cag.2021.09.014},
   year = {2022},
}
@misc{Ahn2019,
   author = {Hyemin Ahn and Jaehun Kim and Kihyun Kim and Songhwai Oh},
   title = {Generative Autoregressive Networks for 3D Dancing Move Synthesis from Music},
   year = {2019},
}
@inproceedings{Huang2022,
   author = {Yuhang Huang and Junjie Zhang and Shuyan Liu and Qian Bao and Dan Zeng and Zhineng Chen and Wu Liu},
   doi = {10.1109/ICASSP43922.2022.9747838},
   journal = {ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
   keywords = {Solid modeling;Three-dimensional displays;Conferences;Music;Signal processing;Transformers;Decoding;3D dance generation;genre-conditioned;modality fusion;music-driven},
   pages = {4858-4862},
   title = {Genre-Conditioned Long-Term 3D Dance Generation Driven by Music},
   year = {2022},
}
@inproceedings{Alemi2017,
   author = {Omid Alemi and Jules Françoise and Philippe Pasquier},
   month = {4},
   title = {GrooveNet: Real-Time Music-Driven Dance Movement Generation using Artificial Neural Networks},
   year = {2017},
}
@inproceedings{Wang2022,
   abstract = {Different people dance in different styles. So when multiple people dance together, the phenomenon of style collaboration occurs: people need to seek common points while reserving differences in various dancing periods. Thus, we introduce a novel Music-driven Group Dance Synthesis task. Compared with single-people dance synthesis explored by most previous works, modeling the style collaboration phenomenon and choreographing for multiple people are more complicated and challenging. Moreover, the lack of sufficient records for conducting multi-people choreography in prior datasets further aggravates this problem. To address these issues, we construct a rich-annotated 3D Multi-Dancer Choreography dataset (MDC) and newly devise a metric SCEU for style collaboration evaluation. To our best knowledge, MDC is the first 3D dance dataset that collects both individual and collaborated music-dance pairs. Based on MDC, we present a novel framework, GroupDancer, consisting of three stages: Dancer Collaboration, Motion Choreography and Motion Transition. The Dancer Collaboration stage determines when and which dancers should collaborate their dancing styles from music. Afterward, the Motion Choreography stage produces a motion sequence for each dancer. Finally, the Motion Transition stage fills the gaps between the motions to achieve fluent and natural group dance. To make GroupDancer trainable from end to end and able to synthesize group dance with style collaboration, we propose mixed training and selective updating strategies. Comprehensive evaluations on the MDC dataset demonstrate that the proposed GroupDancer model can synthesize quite satisfactory group dance synthesis results with style collaboration.},
   author = {Zixuan Wang and Jia Jia and Haozhe Wu and Junliang Xing and Jinghe Cai and Fanbo Meng and Guowen Chen and Yanfeng Wang},
   city = {New York, NY, USA},
   doi = {10.1145/3503161.3548090},
   isbn = {9781450392037},
   journal = {Proceedings of the 30th ACM International Conference on Multimedia},
   keywords = {choreography,group dance synthesis,style collaboration},
   pages = {1138-1146},
   publisher = {Association for Computing Machinery},
   title = {GroupDancer: Music to Multi-People Dance Synthesis with Style Collaboration},
   url = {https://doi.org/10.1145/3503161.3548090},
   year = {2022},
}
@misc{,
   abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension , and summarization, are typically approached with supervised learning on task-specific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset-matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
   author = {Alec Radford and Jeffrey Wu and Rewon Child and David Luan and Dario Amodei and Ilya Sutskever},
   title = {Language Models are Unsupervised Multitask Learners},
   url = {https://github.com/codelucas/newspaper},
}
@inproceedings{Wallace2021,
   abstract = {Through dance, a wide range of emotions can be expressed. As virtual agents and robots continue to become part of our daily lives, the need for them to efficiently convey emotion and intent increases. When trained to dance, to what extent can AI learn to model the tacit mappings between sound and motion? Here, we explore the creative capacity of a generative model trained on 3D motion capture recordings of improvised dance. We perform a perceptual judgment experiment wherein respondents rate movement generated by our model as well as human performances. While the sound-motion mappings remain somewhat elusive, particularly when compared to examples of human dance, our study shows that in certain aspects related to perceived dance-likeness and expressivity, the model successfully mimics human dance movement. By employing a perceptual study to evaluate our generative model, we aim to further our ability to understand the affordances and limitations of creative AI.},
   author = {Benedikte Wallace and Charles P Martin and Jim T\o\{\}rresen and Kristian Nymoen},
   city = {New York, NY, USA},
   doi = {10.1145/3450741.3465245},
   isbn = {9781450383769},
   journal = {Proceedings of the 13th Conference on Creativity and Cognition},
   keywords = {Dance,Embodied Music Cognition,Generative AI,Mixture Density Networks,Perceptual judgement experiment},
   publisher = {Association for Computing Machinery},
   title = {Learning Embodied Sound-Motion Mappings: Evaluating AI-Generated Dance Improvisation},
   url = {https://doi.org/10.1145/3450741.3465245},
   year = {2021},
}
@inproceedings{Liu2019,
   abstract = {Computers that are able to collaboratively improvise movement with humans could have an impact on a variety of application domains, ranging from improving procedural animation in game environments to fostering human-computer co-creativity. Enabling real-time movement improvisation requires equipping computers with strategies for learning and understanding movement. Most existing research focuses on gesture classification, which does not facilitate the learning of new gestures, thereby limiting the creative capacity of computers. In this paper, we explore how to develop a gesture clustering pipeline that facilitates reasoning about arbitrary novel movements in real-time. We describe the implementation of this pipeline within the context of LuminAI, a system in which humans can collaboratively improvise movements together with an AI agent. A preliminary evaluation indicates that our pipeline is capable of efficiently clustering similar gestures together, but further work is necessary to fully assess the pipeline's ability to meaningfully cluster complex movements.},
   author = {Lucas Liu and Duri Long and Swar Gujrania and Brian Magerko},
   doi = {10.1145/3347122.3347127},
   isbn = {9781450376549},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Clustering,Co-creative,Dance,Dimensionality reduction,Dynamic programming,Kinect,Lifelong machine learning,Machine learning,Motion capture,Movement,Pre-processing},
   month = {10},
   publisher = {Association for Computing Machinery},
   title = {Learning movement through human-computer co-creative improvisation},
   year = {2019},
}
@article{Li2020,
   author = {Jiaman Li and Yihang Yin and Hang Chu and Yi Zhou and Tingwu Wang and Sanja Fidler and Hao Li},
   title = {Learning to Generate Diverse Dance Motions with Transformer},
   year = {2020},
}
@article{,
   abstract = {Technologies for sensing movement are expanding toward everyday use in virtual reality, gaming, and artistic practices. In this context, there is a need for methodologies to help designers and users create meaningful movement experiences. This article discusses a user-centered approach for the design of interactive auditory feedback using interactive machine learning. We discuss Mapping through Interaction, a method for crafting sonic interactions from corporeal demonstrations of embodied associations between motion and sound. It uses an interactive machine learning approach to build the mapping from user demonstrations, emphasizing an iterative design process that integrates acted and interactive experiences of the relationships between movement and sound. We examine Gaussian Mixture Regression and Hidden Markov Regression for continuous movement recognition and real-time sound parameter generation. We illustrate and evaluate this approach through an application in which novice users can create interactive sound feedback based on coproduced gestures and vocalizations. Results indicate that Gaussian Mixture Regression and Hidden Markov Regression can efficiently learn complex motion-sound mappings from few examples.},
   author = {Jules Françoise and Frédéric Bevilacqua},
   city = {New York, NY, USA},
   doi = {10.1145/3211826},
   issn = {2160-6455},
   issue = {2},
   journal = {ACM Trans. Interact. Intell. Syst.},
   keywords = {Interactive machine learning,movement,programming-by-demonstration,sonification,sound and music computing,user-centered design},
   month = {6},
   publisher = {Association for Computing Machinery},
   title = {Motion-Sound Mapping through Interaction: An Approach to User-Centered Design of Auditory Feedback Using Machine Learning},
   volume = {8},
   url = {https://doi.org/10.1145/3211826},
   year = {2018},
}
@inproceedings{Bisig2022,
   author = {Daniel Bisig and Ephraim Wegner},
   month = {4},
   title = {Puppeteering AI -Interactive Control of an Artificial Dancer},
   year = {2022},
}
@inproceedings{Long2020,
   abstract = {LuminAI is an art installation in which participants can improvise movements with an AI dance partner. In this practice work, we will present the LuminAI installation as well as two visualization tools that interactively demonstrate how the LuminAI agent reasons about movement using both bottom-up learned knowledge and top-down domain knowledge. Participants will first be invited to interact with the LuminAI installation, where they can improvise movement with an AI agent projected onto a screen. They can then see how LuminAI learns relationships between gestures by interacting with MoViz, a visualization in which participants can explore the agent's gesture memory and qualitatively compare the efficacy of unsupervised learning algorithms at clustering gestures. Finally, participants will be invited to interact with a third tool, where they can explore how LuminAI applies top-down domain knowledge to gesture reasoning. Participants will be able to interactively explore how LuminAI uses Laban Movement Analysis's conception of Space to analyze learned movements in terms of the geometric properties of Laban's icosahedron and manipulate these properties to transform and generate new movements. The two visualization tools both represent novel approaches to understanding and analyzing improvisational movement in creative domains.},
   author = {Duri Long and Lucas Liu and Swar Gujrania and Cassandra Naomi and Brian Magerko},
   doi = {10.1145/3401956.3404258},
   isbn = {9781450375054},
   journal = {ACM International Conference Proceeding Series},
   keywords = {Laban movement analysis,artificial intelligence,computational creativity,explainable AI,gesture,gesture clustering,motion analysis,movement improvisation,unsupervised learning,visualization},
   month = {7},
   publisher = {Association for Computing Machinery},
   title = {Visualizing Improvisation in LuminAI, an AI Partner for Co-Creative Dance},
   year = {2020},
}
